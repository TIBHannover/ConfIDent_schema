import glob
import json
import os
import pprint
import argparse
import dataclasses
from typing import Tuple, List, Dict, Any

from datacite_schema import generated as dc
from datetime import date
from datacite import DataCiteRESTClient, schema42

# Initialize script arguments
p = argparse.ArgumentParser()
p.add_argument("--user", "-u", help="sets username of DataCite account")
p.add_argument("--password", "-pw", help="sets password of DataCite account")
p.add_argument("--prefix", "-p", help="sets prefix of DataCite account")
p.add_argument(
    "--status",
    "-s",
    default="draft",
    help="set the status of the DOI, an ENUM with possible values: 'draft' or 'publish'",
)
p.add_argument(
    "--input_type", "-i", default="events", help="either 'events' or 'series'"
)
p.add_argument("--verbose", "-v", action="store_true", help="debug mode")
args = p.parse_args()

# Initialize Pretty Printer for better debugging
pp = pprint.PrettyPrinter(indent=2)

# Initialize the MDS client.
d = DataCiteRESTClient(
    username=args.user, password=args.password, prefix=args.prefix, test_mode=True
)
# Initialize Constants
TODAY = str(date.today())
ROOT = os.path.join(os.path.dirname(__file__), "..")
DATA_DIR = os.path.join(ROOT, "src", "data", "examples")
DATA_FILES = glob.glob(os.path.join(DATA_DIR, "*.json"))
TEST_SAMPLE = glob.glob(os.path.join(DATA_DIR, "test.json"))


def load(json_file_path: list[str]) -> list[dict]:
    """Load SMW JSON files that contain a list of records designated to be registered or updated with DataCite.

    :param json_file_path: The filepath of the JSON files
    :return: a list of SMW conform dicts that hold event or series metadata

    The input JSON was created under https://www.confident-conference.org/index.php?title=Special:Ask&q=%5B%5BConcept%3AEvents%5D%5D%5B%5BHas+subobject.Organization%3A%3A%2B%5D%5D&p=format%3Dbroadtable&po=%3FAcademic+Field%3Dsubject%0A%3FAcronym%3Dacronym%0A%3FTitle%3Dtitle%0A%3FHas+subobject.Organization%3Dcreator%0A%3FIn+Event+Series%3Din_series%0A%3FStart+Date%3Dstart_date%0A%3FEnd+Date%3Dend_date%0A%3FEvent+Mode%3Devent_mode%0A%3FVenue%3Dvenue%0A%3FCity%3Dcity%0A%3FRegion%3Dregion%0A%3FCountry%3Dcountry%0A%3FOfficial+Website%3Dwebsite%0A%3FDOI%3Ddoi%0A&sort=&order=asc&offset=0&limit=50&eq=yes#search
    """
    batch = []
    for path in json_file_path:
        with open(path, encoding="utf-8") as f:
            smw_data = json.load(f)["results"]
        batch.append(smw_data)
        filename = path.split("\\")[-1]
        print(f'---> Loaded {len(smw_data)} records from "{filename}"')
    return batch


def transform(batch: list) -> tuple[list[dict[str, Any]], list[dict[str, Any]]]:
    """Transform the SMW JSON records into DataCite conform JSON.

    :param batch: a list of SMW conform dicts that hold event or series metadata
    :return: a list of DataCite conform dicts that hold event or series metadata

    The DataCite JSON structure was generated by using the JSON view (dc_json.json) of a current test record in Fabrica.
    This was then converted to Pydantic dataclasses (generated.py) using the 'xsdata' CLI package and the command:
        poetry run xsdata dc_json.json --structure-style single-package --relative-imports --order -r /
            --compound-fields --unnest-classes --output pydantic
        (the autogenerated __init__.py of this package needs to be edited to "from .generated import")
    """
    # a list that holds DataCite conform dicts with records that need to have a DOI minted
    to_update = []
    # a list that holds DataCite conform dicts with records that need to have their DOI metadata updated
    to_mint = []
    # process SMW dict batch
    for events in batch:
        for event in events:
            r = dc.Attributes(
                publisher="German National Library of Science and Technology (TIB)",
            )
            if args.input_type == "events":
                r.types = dc.Types(
                    resourceType="Academic Event", resourceTypeGeneral="Event"
                )
            elif args.input_type == "series":
                r.types = dc.Types(
                    resourceType="Academic Event Series",
                    resourceTypeGeneral="Collection",
                )
            r.url = events[event]["fullurl"].replace(":443", "")
            r.titles.append(
                dc.Titles(
                    lang="English", title="-".join(events[event]["printouts"]["title"])
                )
            )
            if events[event]["printouts"]["acronym"]:
                r.titles.append(
                    dc.Titles(
                        lang="English",
                        title="-".join(events[event]["printouts"]["acronym"]),
                        titleType="AlternativeTitle",
                    )
                )
            if events[event]["printouts"]["in_series"]:
                r.relatedIdentifiers.append(
                    dc.RelatedIdentifiers(
                        relatedIdentifier=events[event]["printouts"]["in_series"][0][
                            "fullurl"
                        ],
                        resourceTypeGeneral="Collection",
                        relationType="IsPartOf",
                        relatedIdentifierType="URL",
                    )
                )
            if events[event]["printouts"]["creator"]:
                for creator in events[event]["printouts"]["creator"]:
                    r.creators.append(
                        dc.Creators(
                            name=creator["displaytitle"], nameType="Organizational"
                        )
                    )
            start_date = (
                events[event]["printouts"]["start_date"][0]["raw"]
                .replace("1/", "")
                .split("/")
            )
            start_date = date(
                year=int(start_date[0]),
                month=int(start_date[1]),
                day=int(start_date[2]),
            )
            r.publicationYear = start_date.year
            r.dates.append(
                dc.Dates(
                    date=str(start_date), dateType="Valid", dateInformation="start date"
                )
            )
            end_date = (
                events[event]["printouts"]["end_date"][0]["raw"]
                .replace("1/", "")
                .split("/")
            )
            end_date = date(
                year=int(end_date[0]), month=int(end_date[1]), day=int(end_date[2])
            )
            r.dates.append(
                dc.Dates(
                    date=str(end_date), dateType="Valid", dateInformation="end date"
                )
            )
            if events[event]["printouts"]["doi"]:
                r.doi = "".join(events[event]["printouts"]["doi"])
                to_update.append(dataclasses.asdict(r))
            else:
                to_mint.append(dataclasses.asdict(r))
    return to_mint, to_update


if __name__ == "__main__":
    print("###########################")
    print("#########   °_°   #########")
    print("###########################")

    records_to_mint, records_to_update = transform(load(TEST_SAMPLE))

    if args.verbose:
        print("For these records a DOI was minted and thus needs to be added to the ConfIDent record:")
        for record in records_to_mint:
            print(" " + record["url"])
        print()
        print("For these records the DOI metadata was updated at DataCite:")
        for record in records_to_update:
            print(" " + record["url"])

    minted_dois = []
    for record in records_to_mint:
        doi = d.public_doi(metadata=record, url=record["url"])
        minted_dois.append(doi)
    print(f'These DOIs were minted for the processed records: {minted_dois}')

    updated_dois = []
    for record in records_to_update:
        doi = d.update_doi(doi=record["doi"], metadata=record)
        updated_dois.append(doi)
    print(f'For these DOIs {updated_dois} the metadata was updated.')

